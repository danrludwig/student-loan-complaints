{"cells":[{"cell_type":"markdown","source":"This is the second notebook I'll be using for my LDA model with the customer complaints concerning the student loan data. I'm going to make a few adustments and hopefully get a better score than I did on my last model. I'm doing a different notebook to compare the model and process to the last one so nothing gets lost. The Coherence Score I got from the last model was 0.4441 (which was with 8 topics), which isn't all that good, so here I will see if I can get it above 0.6.","metadata":{"cell_id":"00000-c8285f39-f9c8-4885-b0d7-dc2df016e2ce"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-8778f643-43e5-4073-9170-52da02e57790","output_cleared":false,"source_hash":"2c1c1732","execution_millis":4224,"execution_start":1604273087335},"source":"!pip install nltk==3.5","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting nltk==3.5\n  Downloading nltk-3.5.zip (1.4 MB)\n\u001b[K     |████████████████████████████████| 1.4 MB 13.8 MB/s \n\u001b[?25hCollecting click\n  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n\u001b[K     |████████████████████████████████| 82 kB 2.1 MB/s \n\u001b[?25hRequirement already satisfied: joblib in /opt/venv/lib/python3.7/site-packages (from nltk==3.5) (0.17.0)\nCollecting regex\n  Downloading regex-2020.10.28-cp37-cp37m-manylinux2010_x86_64.whl (666 kB)\n\u001b[K     |████████████████████████████████| 666 kB 69.2 MB/s \n\u001b[?25hCollecting tqdm\n  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n\u001b[K     |████████████████████████████████| 70 kB 21.6 MB/s \n\u001b[?25hBuilding wheels for collected packages: nltk\n  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434674 sha256=3d7e4b6e695d2923d9831c337bb28697ef3ef694daf24eb53a313e03890adb73\n  Stored in directory: /home/jovyan/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\nSuccessfully built nltk\nInstalling collected packages: click, regex, tqdm, nltk\nSuccessfully installed click-7.1.2 nltk-3.5 regex-2020.10.28 tqdm-4.51.0\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00001-abd040ed-9795-4af3-bcfd-49edf428607b","output_cleared":false,"source_hash":"c3d32871","execution_millis":757,"execution_start":1604273091563},"source":"# Make sure to have nltk and stopwords downloaded\nimport nltk; nltk.download(\"stopwords\")","execution_count":null,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n","output_type":"stream"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-792e5e8e-3a63-44fc-8f64-10575fe68a39","output_cleared":false,"source_hash":"97c41afa","execution_millis":3515,"execution_start":1604273092325},"source":"!pip install gensim==3.8.3","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting gensim==3.8.3\n  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n\u001b[K     |████████████████████████████████| 24.2 MB 19.9 MB/s \n\u001b[?25hRequirement already satisfied: six>=1.5.0 in /opt/venv/lib/python3.7/site-packages (from gensim==3.8.3) (1.15.0)\nRequirement already satisfied: numpy>=1.11.3 in /opt/venv/lib/python3.7/site-packages (from gensim==3.8.3) (1.19.2)\nCollecting smart-open>=1.8.1\n  Downloading smart_open-3.0.0.tar.gz (113 kB)\n\u001b[K     |████████████████████████████████| 113 kB 82.2 MB/s \n\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /opt/venv/lib/python3.7/site-packages (from gensim==3.8.3) (1.5.2)\nRequirement already satisfied: requests in /opt/venv/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim==3.8.3) (2.24.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (2020.6.20)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (1.25.10)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (2.10)\nBuilding wheels for collected packages: smart-open\n  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107095 sha256=568fe9eb61bbbba91667a9a2ef3501c6d4631b6792f7b0d3ceff77661d64b4c8\n  Stored in directory: /home/jovyan/.cache/pip/wheels/83/a6/12/bf3c1a667bde4251be5b7a3368b2d604c9af2105b5c1cb1870\nSuccessfully built smart-open\nInstalling collected packages: smart-open, gensim\nSuccessfully installed gensim-3.8.3 smart-open-3.0.0\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-9931e004-1185-4b7e-b36b-4dfd79617d09","output_cleared":false,"source_hash":"f96e8c46","execution_millis":4200,"execution_start":1604273095846},"source":"!pip install spacy==2.3.2","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting spacy==2.3.2\n  Downloading spacy-2.3.2-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n\u001b[K     |████████████████████████████████| 9.9 MB 12.4 MB/s \n\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/venv/lib/python3.7/site-packages (from spacy==2.3.2) (4.51.0)\nCollecting plac<1.2.0,>=0.9.6\n  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\nCollecting catalogue<1.1.0,>=0.0.7\n  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\nCollecting blis<0.5.0,>=0.4.0\n  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n\u001b[K     |████████████████████████████████| 3.7 MB 73.5 MB/s \n\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0\n  Downloading wasabi-0.8.0-py3-none-any.whl (23 kB)\nCollecting srsly<1.1.0,>=1.0.2\n  Downloading srsly-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (185 kB)\n\u001b[K     |████████████████████████████████| 185 kB 97.1 MB/s \n\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/venv/lib/python3.7/site-packages (from spacy==2.3.2) (2.24.0)\nRequirement already satisfied: setuptools in /opt/venv/lib/python3.7/site-packages (from spacy==2.3.2) (50.3.0)\nCollecting cymem<2.1.0,>=2.0.2\n  Downloading cymem-2.0.3-cp37-cp37m-manylinux1_x86_64.whl (32 kB)\nCollecting thinc==7.4.1\n  Downloading thinc-7.4.1-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n\u001b[K     |████████████████████████████████| 2.1 MB 66.3 MB/s \n\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n  Downloading murmurhash-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (19 kB)\nRequirement already satisfied: numpy>=1.15.0 in /opt/venv/lib/python3.7/site-packages (from spacy==2.3.2) (1.19.2)\nCollecting preshed<3.1.0,>=3.0.2\n  Downloading preshed-3.0.2-cp37-cp37m-manylinux1_x86_64.whl (118 kB)\n\u001b[K     |████████████████████████████████| 118 kB 90.4 MB/s \n\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.2) (2.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (2020.6.20)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (1.25.10)\nRequirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (2.10)\nRequirement already satisfied: zipp>=0.5 in /opt/venv/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.3.2) (3.3.0)\nInstalling collected packages: plac, catalogue, blis, wasabi, srsly, cymem, murmurhash, preshed, thinc, spacy\nSuccessfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.3.2 srsly-1.0.2 thinc-7.4.1 wasabi-0.8.0\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-11d45b89-96ac-4187-b4e2-013372dc6d07","output_cleared":false,"source_hash":"aa037d08","execution_millis":4644,"execution_start":1604273100052},"source":"!pip install pyLDAvis==2.1.2","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting pyLDAvis==2.1.2\n  Downloading pyLDAvis-2.1.2.tar.gz (1.6 MB)\n\u001b[K     |████████████████████████████████| 1.6 MB 18.8 MB/s \n\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /opt/venv/lib/python3.7/site-packages (from pyLDAvis==2.1.2) (0.35.1)\nRequirement already satisfied: numpy>=1.9.2 in /opt/venv/lib/python3.7/site-packages (from pyLDAvis==2.1.2) (1.19.2)\nRequirement already satisfied: scipy>=0.18.0 in /opt/venv/lib/python3.7/site-packages (from pyLDAvis==2.1.2) (1.5.2)\nRequirement already satisfied: pandas>=0.17.0 in /opt/venv/lib/python3.7/site-packages (from pyLDAvis==2.1.2) (1.0.5)\nRequirement already satisfied: joblib>=0.8.4 in /opt/venv/lib/python3.7/site-packages (from pyLDAvis==2.1.2) (0.17.0)\nRequirement already satisfied: jinja2>=2.7.2 in /opt/venv/lib/python3.7/site-packages (from pyLDAvis==2.1.2) (2.11.2)\nCollecting numexpr\n  Downloading numexpr-2.7.1-cp37-cp37m-manylinux1_x86_64.whl (162 kB)\n\u001b[K     |████████████████████████████████| 162 kB 92.1 MB/s \n\u001b[?25hCollecting pytest\n  Downloading pytest-6.1.2-py3-none-any.whl (272 kB)\n\u001b[K     |████████████████████████████████| 272 kB 90.7 MB/s \n\u001b[?25hCollecting future\n  Downloading future-0.18.2.tar.gz (829 kB)\n\u001b[K     |████████████████████████████████| 829 kB 76.3 MB/s \n\u001b[?25hCollecting funcy\n  Downloading funcy-1.15-py2.py3-none-any.whl (32 kB)\nRequirement already satisfied: pytz>=2017.2 in /opt/venv/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2020.1)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/venv/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2.8.1)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/venv/lib/python3.7/site-packages (from jinja2>=2.7.2->pyLDAvis==2.1.2) (1.1.1)\nRequirement already satisfied: attrs>=17.4.0 in /opt/venv/lib/python3.7/site-packages (from pytest->pyLDAvis==2.1.2) (20.2.0)\nRequirement already satisfied: packaging in /opt/venv/lib/python3.7/site-packages (from pytest->pyLDAvis==2.1.2) (20.4)\nCollecting pluggy<1.0,>=0.12\n  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\nCollecting toml\n  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\nCollecting iniconfig\n  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\nRequirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /opt/venv/lib/python3.7/site-packages (from pytest->pyLDAvis==2.1.2) (2.0.0)\nCollecting py>=1.8.2\n  Downloading py-1.9.0-py2.py3-none-any.whl (99 kB)\n\u001b[K     |████████████████████████████████| 99 kB 17.4 MB/s \n\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/venv/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.17.0->pyLDAvis==2.1.2) (1.15.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/venv/lib/python3.7/site-packages (from packaging->pytest->pyLDAvis==2.1.2) (2.4.7)\nRequirement already satisfied: zipp>=0.5 in /opt/venv/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->pyLDAvis==2.1.2) (3.3.0)\nBuilding wheels for collected packages: pyLDAvis, future\n  Building wheel for pyLDAvis (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=02dcaf411ea6e23944d89a8a9a62d055d1b8d8c2c5387a79e74f56923a48c2fb\n  Stored in directory: /home/jovyan/.cache/pip/wheels/3b/fb/41/e32e5312da9f440d34c4eff0d2207b46dc9332a7b931ef1e89\n  Building wheel for future (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=919c6c4df92b6beb24df703f79e7e4b8f98b183e3a7a359e34daa4643919d1d2\n  Stored in directory: /home/jovyan/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\nSuccessfully built pyLDAvis future\nInstalling collected packages: numexpr, pluggy, toml, iniconfig, py, pytest, future, funcy, pyLDAvis\n  Attempting uninstall: pluggy\n    Found existing installation: pluggy 1.0.0.dev0\n    Uninstalling pluggy-1.0.0.dev0:\n      Successfully uninstalled pluggy-1.0.0.dev0\nSuccessfully installed funcy-1.15 future-0.18.2 iniconfig-1.1.1 numexpr-2.7.1 pluggy-0.13.1 py-1.9.0 pyLDAvis-2.1.2 pytest-6.1.2 toml-0.10.2\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00002-c34e2df1-cb68-4e40-948c-ac556cff1ba1","output_cleared":false,"source_hash":"b08c6249","execution_millis":729,"execution_start":1604273104702},"source":"# Import needed packages\n\nimport re\nimport numpy\nimport pandas as pd\nfrom pprint import pprint\n\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\nimport spacy\n\nimport pyLDAvis\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport logging\nlogging.basicConfig(format=\"%(asctime)s: %(levelname)s : %(message)s\", level=logging.ERROR)\n\nimport warnings \nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00003-1b5a8852-f2c3-47f2-bc6e-6581d7ee1fee","output_cleared":false,"source_hash":"d8fcef0b","execution_millis":1,"execution_start":1604273105438},"source":"# NLTK stop words\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words(\"english\")\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00004-269f3c5e-1920-49c1-9cd1-f19ee4a0896f","output_cleared":false,"source_hash":"e34e15dd","execution_millis":415,"execution_start":1604273105442},"source":"# Get data\ndf = pd.read_csv('../data/text_analysis_data.csv')\ndf.head()","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":5,"column_count":13,"columns":[{"name":"Date received","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"2020-05-19","count":1},{"name":"2020-02-06","count":1},{"name":"3 others","count":3}]}},{"name":"Sub-product","dtype":"object","stats":{"unique_count":2,"nan_count":0,"categories":[{"name":"Federal student loan servicing","count":4},{"name":"Private student loan","count":1}]}},{"name":"Issue","dtype":"object","stats":{"unique_count":3,"nan_count":0,"categories":[{"name":"Dealing with your lender or servicer","count":3},{"name":"Incorrect information on your report","count":1},{"name":"Problem with a credit reporting company's investigation into an existing problem","count":1}]}},{"name":"Sub-issue","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"Received bad information about your loan","count":1},{"name":"Account status incorrect","count":1},{"name":"3 others","count":3}]}},{"name":"Consumer complaint narrative","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"When I was applying for my loan my XXXX account and XXXX XXXX  did not correctly communicate with each other. This is an issue because they offer a 0.25 % rate deduction for being on autopay and it was not showing up in my account. They told me to go through the application anyway and that once the account was opened that I could add autopay and receive the discount that way. \r\n\r\nSince I have had the account opened I have called their call center at least 4 times trying to receive my autopay discount and each of the first 3 times I was told it was going to be applied and still have not seen it. Additionally, this last time I called about 3 weeks ago and I asked to speak to a manager and was told it would take 10 days for them to get back to me. I still have yet to hear back from them 15+ business days later. \r\n\r\nI was told that I would be receiving the autopay discount and am not receiving it so I opened this account up with this company lying to me about the rate of the loan I was going to receive. Its not as if I dont have autopay initiates, I have had 2 autopays go through so far. \r\n\r\nI have an additional issue where they told me that my rate was going to be based off the 1 month libor rate as published in the wsj on the XXXX of that month. None of the rates I have received thus far match that rate. \r\n\r\nI dont really know what to do, I have tried contacting them so many times and while the people on the phone seem helpful at the time of me talking to them nothing seems to get done once I hang up.","count":1},{"name":"I'm on a deferred payment plan t never ; late","count":1},{"name":"3 others","count":3}]}},{"name":"Company","dtype":"object","stats":{"unique_count":3,"nan_count":0,"categories":[{"name":"AES/PHEAA","count":3},{"name":"Figure Technologies, Inc","count":1},{"name":"Nelnet, Inc.","count":1}]}},{"name":"State","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"NJ","count":1},{"name":"TX","count":1},{"name":"3 others","count":3}]}},{"name":"Tags","dtype":"object","stats":{"unique_count":1,"nan_count":0,"categories":[{"name":"None","count":5}]}},{"name":"Company response to consumer","dtype":"object","stats":{"unique_count":2,"nan_count":0,"categories":[{"name":"Closed with explanation","count":4},{"name":"Closed with non-monetary relief","count":1}]}},{"name":"Timely response?","dtype":"object","stats":{"unique_count":1,"nan_count":0,"categories":[{"name":"Yes","count":5}]}},{"name":"Consumer disputed?","dtype":"object","stats":{"unique_count":0,"nan_count":5,"categories":[{"name":"Missing","count":5}]}},{"name":"month","dtype":"int64","stats":{"unique_count":4,"nan_count":0,"min":1,"max":12,"histogram":[{"bin_start":1,"bin_end":2.1,"count":3},{"bin_start":2.1,"bin_end":3.2,"count":0},{"bin_start":3.2,"bin_end":4.300000000000001,"count":0},{"bin_start":4.300000000000001,"bin_end":5.4,"count":1},{"bin_start":5.4,"bin_end":6.5,"count":0},{"bin_start":6.5,"bin_end":7.6000000000000005,"count":0},{"bin_start":7.6000000000000005,"bin_end":8.700000000000001,"count":0},{"bin_start":8.700000000000001,"bin_end":9.8,"count":0},{"bin_start":9.8,"bin_end":10.9,"count":0},{"bin_start":10.9,"bin_end":12,"count":1}]}},{"name":"year","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":2019,"max":2020,"histogram":[{"bin_start":2019,"bin_end":2019.1,"count":1},{"bin_start":2019.1,"bin_end":2019.2,"count":0},{"bin_start":2019.2,"bin_end":2019.3,"count":0},{"bin_start":2019.3,"bin_end":2019.4,"count":0},{"bin_start":2019.4,"bin_end":2019.5,"count":0},{"bin_start":2019.5,"bin_end":2019.6,"count":0},{"bin_start":2019.6,"bin_end":2019.7,"count":0},{"bin_start":2019.7,"bin_end":2019.8,"count":0},{"bin_start":2019.8,"bin_end":2019.9,"count":0},{"bin_start":2019.9,"bin_end":2020,"count":4}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"Date received":"2020-05-19","Sub-product":"Private student loan","Issue":"Dealing with your lender or servicer","Sub-issue":"Received bad information about your loan","Consumer complaint narrative":"When I was applying for my loan my XXXX account and XXXX XXXX  did not correctly communicate with each other. This is an issue because they offer a 0.25 % rate deduction for being on autopay and it was not showing up in my account. They told me to go through the application anyway and that once the account was opened that I could add autopay and receive the discount that way. \r\n\r\nSince I have had the account opened I have called their call center at least 4 times trying to receive my autopay discount and each of the first 3 times I was told it was going to be applied and still have not seen it. Additionally, this last time I called about 3 weeks ago and I asked to speak to a manager and was told it would take 10 days for them to get back to me. I still have yet to hear back from them 15+ business days later. \r\n\r\nI was told that I would be receiving the autopay discount and am not receiving it so I opened this account up with this company lying to me about the rate of the loan I was going to receive. Its not as if I dont have autopay initiates, I have had 2 autopays go through so far. \r\n\r\nI have an additional issue where they told me that my rate was going to be based off the 1 month libor rate as published in the wsj on the XXXX of that month. None of the rates I have received thus far match that rate. \r\n\r\nI dont really know what to do, I have tried contacting them so many times and while the people on the phone seem helpful at the time of me talking to them nothing seems to get done once I hang up.","Company":"Figure Technologies, Inc","State":"NJ","Tags":"None","Company response to consumer":"Closed with explanation","Timely response?":"Yes","Consumer disputed?":"nan","month":5,"year":2020,"_deepnote_index_column":0},{"Date received":"2020-02-06","Sub-product":"Federal student loan servicing","Issue":"Incorrect information on your report","Sub-issue":"Account status incorrect","Consumer complaint narrative":"I'm on a deferred payment plan t never ; late","Company":"Nelnet, Inc.","State":"TX","Tags":"None","Company response to consumer":"Closed with explanation","Timely response?":"Yes","Consumer disputed?":"nan","month":2,"year":2020,"_deepnote_index_column":1},{"Date received":"2020-02-08","Sub-product":"Federal student loan servicing","Issue":"Dealing with your lender or servicer","Sub-issue":"Problem with customer service","Consumer complaint narrative":"I have attempted multiple times to contact FEDLOAN via their online message regarding payback and qualifying payments for Public health forgiveness program. They never respond and never show proof of messages. If you call to talk to agent, there is no paper trail and calls last for such a long time it isn't possible to call with questions. \r\nWhen you get to discuss loan with agent the details change per agent. \r\n\r\nI simply want my total amount of payments to be accurate. I have had automated debit since the onset and on time 100 %. They did not give me credit for 3 payments because I paid more than the lowest amount thus fedloan took it upon themselves to just use the extra amount to not add to the principal as i requested but instead delay counting the amount of months for me. \r\n\r\nThey will not give me written information regarding that date. I consolidated XX/XX/XXXX and instead of XX/XX/XXXX being the 10 year forgiveness date it has been changed to XX/XX/XXXX arbitrarily. \r\n\r\nI have talked to several people and it seems to be a common scam by FEDLOANS to delay the 10 year time frame to months and even years of more payments.","Company":"AES/PHEAA","State":"KY","Tags":"None","Company response to consumer":"Closed with non-monetary relief","Timely response?":"Yes","Consumer disputed?":"nan","month":2,"year":2020,"_deepnote_index_column":2},{"Date received":"2020-01-21","Sub-product":"Federal student loan servicing","Issue":"Dealing with your lender or servicer","Sub-issue":"Trouble with how payments are being handled","Consumer complaint narrative":"I was divorced in 2004 and I agreed to take the school loans in my divorce. Our loans are consolidated. Therefore, they are wanting to take my tax return and my ex-husbands tax return including his new wife 's income and base my monthly payment on that total income. I have spoke with numerous people and they quote me from {$500.00} a month to {$1000.00}. I am a single mother and have no means to pay that kind of payment. My credit score went down 43 points in one month due to my PHEAA student loan account balance increasing by {$100.00} from {$29000.00} to {$29000.00}.","Company":"AES/PHEAA","State":"OK","Tags":"None","Company response to consumer":"Closed with explanation","Timely response?":"Yes","Consumer disputed?":"nan","month":1,"year":2020,"_deepnote_index_column":3},{"Date received":"2019-12-04","Sub-product":"Federal student loan servicing","Issue":"Problem with a credit reporting company's investigation into an existing problem","Sub-issue":"Their investigation did not fix an error on your report","Consumer complaint narrative":"This particular account situation that is lately filing on my own credit document has a seriously unfavorable relation to my personal ability to obtain a present loan application. I highly recommend you generate verification that FED LOAN SERVICING has been reported completely in accordance with the Fair Credit Reporting Act regulations, it's really a serious problem to mis-report. Moreconfirmation of the aforesaid item too. My proper request mustover, I was never 30 days/60 days/120 days late in any of my payments and I'm not greatly tuned in to the date opened so I prefer to ask you be investigated as soon as possible and confirmed to be correct. Thanks!","Company":"AES/PHEAA","State":"FL","Tags":"None","Company response to consumer":"Closed with explanation","Timely response?":"Yes","Consumer disputed?":"nan","month":12,"year":2019,"_deepnote_index_column":4}],"rows_bottom":null},"text/plain":"  Date received                     Sub-product  \\\n0    2020-05-19            Private student loan   \n1    2020-02-06  Federal student loan servicing   \n2    2020-02-08  Federal student loan servicing   \n3    2020-01-21  Federal student loan servicing   \n4    2019-12-04  Federal student loan servicing   \n\n                                               Issue  \\\n0               Dealing with your lender or servicer   \n1               Incorrect information on your report   \n2               Dealing with your lender or servicer   \n3               Dealing with your lender or servicer   \n4  Problem with a credit reporting company's inve...   \n\n                                           Sub-issue  \\\n0           Received bad information about your loan   \n1                           Account status incorrect   \n2                      Problem with customer service   \n3        Trouble with how payments are being handled   \n4  Their investigation did not fix an error on yo...   \n\n                        Consumer complaint narrative  \\\n0  When I was applying for my loan my XXXX accoun...   \n1      I'm on a deferred payment plan t never ; late   \n2  I have attempted multiple times to contact FED...   \n3  I was divorced in 2004 and I agreed to take th...   \n4  This particular account situation that is late...   \n\n                    Company State  Tags     Company response to consumer  \\\n0  Figure Technologies, Inc    NJ  None          Closed with explanation   \n1              Nelnet, Inc.    TX  None          Closed with explanation   \n2                 AES/PHEAA    KY  None  Closed with non-monetary relief   \n3                 AES/PHEAA    OK  None          Closed with explanation   \n4                 AES/PHEAA    FL  None          Closed with explanation   \n\n  Timely response? Consumer disputed?  month  year  \n0              Yes                NaN      5  2020  \n1              Yes                NaN      2  2020  \n2              Yes                NaN      2  2020  \n3              Yes                NaN      1  2020  \n4              Yes                NaN     12  2019  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date received</th>\n      <th>Sub-product</th>\n      <th>Issue</th>\n      <th>Sub-issue</th>\n      <th>Consumer complaint narrative</th>\n      <th>Company</th>\n      <th>State</th>\n      <th>Tags</th>\n      <th>Company response to consumer</th>\n      <th>Timely response?</th>\n      <th>Consumer disputed?</th>\n      <th>month</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-05-19</td>\n      <td>Private student loan</td>\n      <td>Dealing with your lender or servicer</td>\n      <td>Received bad information about your loan</td>\n      <td>When I was applying for my loan my XXXX accoun...</td>\n      <td>Figure Technologies, Inc</td>\n      <td>NJ</td>\n      <td>None</td>\n      <td>Closed with explanation</td>\n      <td>Yes</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-02-06</td>\n      <td>Federal student loan servicing</td>\n      <td>Incorrect information on your report</td>\n      <td>Account status incorrect</td>\n      <td>I'm on a deferred payment plan t never ; late</td>\n      <td>Nelnet, Inc.</td>\n      <td>TX</td>\n      <td>None</td>\n      <td>Closed with explanation</td>\n      <td>Yes</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-02-08</td>\n      <td>Federal student loan servicing</td>\n      <td>Dealing with your lender or servicer</td>\n      <td>Problem with customer service</td>\n      <td>I have attempted multiple times to contact FED...</td>\n      <td>AES/PHEAA</td>\n      <td>KY</td>\n      <td>None</td>\n      <td>Closed with non-monetary relief</td>\n      <td>Yes</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-01-21</td>\n      <td>Federal student loan servicing</td>\n      <td>Dealing with your lender or servicer</td>\n      <td>Trouble with how payments are being handled</td>\n      <td>I was divorced in 2004 and I agreed to take th...</td>\n      <td>AES/PHEAA</td>\n      <td>OK</td>\n      <td>None</td>\n      <td>Closed with explanation</td>\n      <td>Yes</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-12-04</td>\n      <td>Federal student loan servicing</td>\n      <td>Problem with a credit reporting company's inve...</td>\n      <td>Their investigation did not fix an error on yo...</td>\n      <td>This particular account situation that is late...</td>\n      <td>AES/PHEAA</td>\n      <td>FL</td>\n      <td>None</td>\n      <td>Closed with explanation</td>\n      <td>Yes</td>\n      <td>NaN</td>\n      <td>12</td>\n      <td>2019</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Now that we've imported the necessary packages we will prepare the data to build and feed into the model.","metadata":{"cell_id":"00005-3d85f90b-b741-472c-87b2-efeffea8d554"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00010-ba503596-2a07-4023-b558-367a3f566ffd","output_cleared":false,"source_hash":"cbf857c1","execution_millis":1616,"execution_start":1604273105866},"source":"!pip install textblob==0.15.3","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting textblob==0.15.3\n  Downloading textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n\u001b[K     |████████████████████████████████| 636 kB 11.5 MB/s \n\u001b[?25hRequirement already satisfied: nltk>=3.1 in /opt/venv/lib/python3.7/site-packages (from textblob==0.15.3) (3.5)\nRequirement already satisfied: click in /opt/venv/lib/python3.7/site-packages (from nltk>=3.1->textblob==0.15.3) (7.1.2)\nRequirement already satisfied: joblib in /opt/venv/lib/python3.7/site-packages (from nltk>=3.1->textblob==0.15.3) (0.17.0)\nRequirement already satisfied: regex in /opt/venv/lib/python3.7/site-packages (from nltk>=3.1->textblob==0.15.3) (2020.10.28)\nRequirement already satisfied: tqdm in /opt/venv/lib/python3.7/site-packages (from nltk>=3.1->textblob==0.15.3) (4.51.0)\nInstalling collected packages: textblob\nSuccessfully installed textblob-0.15.3\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00006-c86e7f42-408c-4134-8a87-e48cec23834c","output_cleared":false,"source_hash":"69b023e5","execution_millis":32789,"execution_start":1604273107490},"source":"!python -m textblob.download_corpora\n# A function to preprocess all rows in a dataframe\ndef preprocess_data(data):\n    # Change all text to lowercase\n    data = data.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    \n    # Remove puctuation\n    data = data.str.replace(\"[^\\w\\s]\",\"\")\n    \n    # Remove stopwords\n    from nltk.corpus import stopwords\n    stop = stopwords.words(\"english\")\n    data = data.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n    \n    # Remove any words like \"xx\" words and numbers\n    data = data.apply(lambda x: \" \".join(x for x in x.split() if \"xx\" not in x))\n    data = data.apply(lambda x: \" \".join(x for x in x.split() if not x.isnumeric()))\n    # This round we decided not to remove common words\n#     freq = pd.Series(\" \".join(data).split()).value_counts()[:10]\n#     freq = list(freq.index)\n#     data = data.apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n    \n    # Lemmatization\n    from textblob import Word\n    data = data.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n    \n    # Return transformed data\n    return data\n\n# Return the data as a list\ndata = preprocess_data(df[\"Consumer complaint narrative\"])","execution_count":null,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/brown.zip.\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package conll2000 to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/conll2000.zip.\n[nltk_data] Downloading package movie_reviews to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/movie_reviews.zip.\nFinished.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00007-edd8d6a7-2df2-49b7-b064-d9959bbb6dc1","output_cleared":false,"source_hash":"be6842a8","execution_millis":231,"execution_start":1604273140350},"source":"# Tokenize the data\ndata = [sub.split() for sub in data] \nprint(data[:3])","execution_count":null,"outputs":[{"name":"stdout","text":"[['applying', 'loan', 'account', 'correctly', 'communicate', 'issue', 'offer', 'rate', 'deduction', 'autopay', 'showing', 'account', 'told', 'go', 'application', 'anyway', 'account', 'opened', 'could', 'add', 'autopay', 'receive', 'discount', 'way', 'since', 'account', 'opened', 'called', 'call', 'center', 'least', 'time', 'trying', 'receive', 'autopay', 'discount', 'first', 'time', 'told', 'going', 'applied', 'still', 'seen', 'additionally', 'last', 'time', 'called', 'week', 'ago', 'asked', 'speak', 'manager', 'told', 'would', 'take', 'day', 'get', 'back', 'still', 'yet', 'hear', 'back', 'business', 'day', 'later', 'told', 'would', 'receiving', 'autopay', 'discount', 'receiving', 'opened', 'account', 'company', 'lying', 'rate', 'loan', 'going', 'receive', 'dont', 'autopay', 'initiate', 'autopays', 'go', 'far', 'additional', 'issue', 'told', 'rate', 'going', 'based', 'month', 'libor', 'rate', 'published', 'wsj', 'month', 'none', 'rate', 'received', 'thus', 'far', 'match', 'rate', 'dont', 'really', 'know', 'tried', 'contacting', 'many', 'time', 'people', 'phone', 'seem', 'helpful', 'time', 'talking', 'nothing', 'seems', 'get', 'done', 'hang'], ['im', 'deferred', 'payment', 'plan', 'never', 'late'], ['attempted', 'multiple', 'time', 'contact', 'fedloan', 'via', 'online', 'message', 'regarding', 'payback', 'qualifying', 'payment', 'public', 'health', 'forgiveness', 'program', 'never', 'respond', 'never', 'show', 'proof', 'message', 'call', 'talk', 'agent', 'paper', 'trail', 'call', 'last', 'long', 'time', 'isnt', 'possible', 'call', 'question', 'get', 'discus', 'loan', 'agent', 'detail', 'change', 'per', 'agent', 'simply', 'want', 'total', 'amount', 'payment', 'accurate', 'automated', 'debit', 'since', 'onset', 'time', 'give', 'credit', 'payment', 'paid', 'lowest', 'amount', 'thus', 'fedloan', 'took', 'upon', 'use', 'extra', 'amount', 'add', 'principal', 'requested', 'instead', 'delay', 'counting', 'amount', 'month', 'give', 'written', 'information', 'regarding', 'date', 'consolidated', 'instead', 'year', 'forgiveness', 'date', 'changed', 'arbitrarily', 'talked', 'several', 'people', 'seems', 'common', 'scam', 'fedloans', 'delay', 'year', 'time', 'frame', 'month', 'even', 'year', 'payment']]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now that we've cleaned and tokenized the data we need to create bigrams and trigrams. Bigrams are two words frequently seen paired together, trigrams are the same but with three words. We will use Gensim's Phrases model to build the bigrams and trigrams.","metadata":{"cell_id":"00008-560f0443-aed1-4216-b5b3-c63316730902"}},{"cell_type":"code","metadata":{"cell_id":"00009-6b628918-8e59-45ca-83fc-6353632c71e2","output_cleared":false,"source_hash":"50a073ab","execution_millis":37301,"execution_start":1604273140617},"source":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data, min_count=5, threshold=20)\ntrigram = gensim.models.Phrases(bigram[data], threshold=20)\n\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[data[0]]])","execution_count":null,"outputs":[{"name":"stdout","text":"['applying', 'loan', 'account', 'correctly', 'communicate', 'issue', 'offer', 'rate', 'deduction', 'autopay', 'showing', 'account', 'told', 'go', 'application', 'anyway', 'account', 'opened', 'could', 'add', 'autopay', 'receive', 'discount', 'way', 'since', 'account', 'opened', 'called', 'call', 'center', 'least', 'time', 'trying', 'receive', 'autopay_discount', 'first', 'time', 'told', 'going', 'applied', 'still', 'seen', 'additionally', 'last', 'time', 'called', 'week_ago', 'asked_speak_manager', 'told', 'would', 'take', 'day', 'get', 'back', 'still', 'yet_hear', 'back', 'business_day', 'later', 'told', 'would', 'receiving', 'autopay_discount', 'receiving', 'opened', 'account', 'company', 'lying', 'rate', 'loan', 'going', 'receive', 'dont', 'autopay', 'initiate', 'autopays', 'go', 'far', 'additional', 'issue', 'told', 'rate', 'going', 'based', 'month', 'libor_rate', 'published', 'wsj', 'month', 'none', 'rate', 'received', 'thus_far', 'match', 'rate', 'dont', 'really', 'know', 'tried_contacting', 'many', 'time', 'people', 'phone', 'seem', 'helpful', 'time', 'talking', 'nothing', 'seems', 'get', 'done', 'hang']\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00010-680f1758-c62c-48fe-85eb-253e61619b2d","output_cleared":false,"source_hash":"889ac24d","execution_millis":0,"execution_start":1604273177919},"source":"# As we can see above, out attempt at lemmatizing the data didn't work. We will just try it again.\n# Here we are defining functions for bigrams, trigrams, and lemmatizing the data\n\ndef make_bigrams(data):\n    return [bigram_mod[doc] for doc in data]\n\ndef make_trigrams(data):\n    return [trigram_mod[bigram_mod[doc]] for doc in data]\n\ndef lemmatization(data, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n    complaints_out = []\n    for complaint in data:\n        doc = nlp(\" \".join(complaint))\n        complaints_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return complaints_out","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00011-e32a6f32-299f-4c3b-9b38-87cf437a1b09","output_cleared":false,"source_hash":"a6a12c71","execution_millis":135259,"execution_start":1604273177919},"source":"!python3 -m spacy download en\n# Now we call the functions we build above\ndata_words_bigrams = make_bigrams(data)\nnlp = spacy.load(\"en\", disable=[\"parser\", \"ner\"])\ndata_lemmatized = lemmatization(data_words_bigrams)\n\nprint(data_lemmatized[:1])","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting en_core_web_sm==2.3.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n\u001b[K     |████████████████████████████████| 12.0 MB 57.9 MB/s \n\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/venv/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\nRequirement already satisfied: setuptools in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (50.3.0)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\nRequirement already satisfied: thinc==7.4.1 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.51.0)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\nRequirement already satisfied: numpy>=1.15.0 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.2)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\nRequirement already satisfied: zipp>=0.5 in /opt/venv/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.3.0)\nBuilding wheels for collected packages: en-core-web-sm\n  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047106 sha256=ab1e5c7a1fdee7daf1dd990d09f32565ff6df9484ca2661fd7bfe8e0cbeeedee\n  Stored in directory: /tmp/pip-ephem-wheel-cache-34_v_uf4/wheels/b7/0d/f0/7ecae8427c515065d75410989e15e5785dd3975fe06e795cd9\nSuccessfully built en-core-web-sm\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-2.3.1\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the model via spacy.load('en_core_web_sm')\n\u001b[38;5;2m✔ Linking successful\u001b[0m\n/opt/venv/lib/python3.7/site-packages/en_core_web_sm -->\n/opt/venv/lib/python3.7/site-packages/spacy/data/en\nYou can now load the model via spacy.load('en')\n[['apply', 'loan', 'account', 'correctly', 'communicate', 'issue', 'offer', 'rate', 'deduction', 'autopay', 'show', 'account', 'tell', 'go', 'application', 'account', 'open', 'could', 'add', 'autopay', 'receive', 'discount', 'way', 'account', 'open', 'call', 'center', 'least', 'time', 'try', 'receive', 'first', 'time', 'tell', 'go', 'apply', 'still', 'see', 'additionally', 'last', 'time', 'call', 'ask', 'speak_manag', 'tell', 'would', 'take', 'day', 'back', 'still', 'yet', 'hear', 'back', 'later', 'tell', 'would', 'receive', 'receive', 'open', 'account', 'company', 'lie', 'rate', 'loan', 'go', 'receive', 'autopay', 'autopay', 'go', 'far', 'additional', 'issue', 'tell', 'rate', 'go', 'base', 'month', 'publish', 'month', 'none', 'rate', 'receive', 'thus_far', 'match', 'rate', 'really', 'know', 'tried_contacte', 'many', 'time', 'people', 'phone', 'seem', 'helpful', 'time', 'talk', 'seem', 'get', 'do']]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"At this point we've created some bi_grams and tri_grams. Now we need to create a dictionary and corpus that's needed for topic modeling.","metadata":{"cell_id":"00012-0a8f0b8f-20ce-46eb-b0f0-26a2986a060f"}},{"cell_type":"code","metadata":{"cell_id":"00013-467518e1-7e78-4be1-a751-1e67d44a0718","output_cleared":false,"source_hash":"6f84a713","execution_millis":4127,"execution_start":1604273313186},"source":"# Create dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ncomplaints = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(complaint) for complaint in complaints]\n\nprint(\"Corpus format: (word_id, work_frequency)\")\nprint(corpus[:1])\nprint()\nprint(\"Readable version of term-frequency:\")\nprint([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]])","execution_count":null,"outputs":[{"name":"stdout","text":"Corpus format: (word_id, work_frequency)\n[[(0, 5), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 4), (8, 2), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 5), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 2), (33, 1), (34, 1), (35, 2), (36, 1), (37, 1), (38, 3), (39, 1), (40, 1), (41, 1), (42, 5), (43, 1), (44, 6), (45, 1), (46, 2), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (52, 5), (53, 1), (54, 5), (55, 1), (56, 1), (57, 1), (58, 2), (59, 1)]]\n\nReadable version of term-frequency:\n[[('account', 5), ('add', 1), ('additional', 1), ('additionally', 1), ('application', 1), ('apply', 2), ('ask', 1), ('autopay', 4), ('back', 2), ('base', 1), ('call', 2), ('center', 1), ('communicate', 1), ('company', 1), ('correctly', 1), ('could', 1), ('day', 1), ('deduction', 1), ('discount', 1), ('do', 1), ('far', 1), ('first', 1), ('get', 1), ('go', 5), ('hear', 1), ('helpful', 1), ('issue', 2), ('know', 1), ('last', 1), ('later', 1), ('least', 1), ('lie', 1), ('loan', 2), ('many', 1), ('match', 1), ('month', 2), ('none', 1), ('offer', 1), ('open', 3), ('people', 1), ('phone', 1), ('publish', 1), ('rate', 5), ('really', 1), ('receive', 6), ('see', 1), ('seem', 2), ('show', 1), ('speak_manag', 1), ('still', 2), ('take', 1), ('talk', 1), ('tell', 5), ('thus_far', 1), ('time', 5), ('tried_contacte', 1), ('try', 1), ('way', 1), ('would', 2), ('yet', 1)]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now that we've prepared the data we are going to create and use a new function to test the other hypertunning parameters for an lda model, hopefully find one that's more accurate.","metadata":{"cell_id":"00014-e55e499b-ab88-44af-9ae9-d2979b7dea6a"}},{"cell_type":"code","metadata":{"cell_id":"00015-9bbfeb13-2060-4603-b05f-e349ef322780","output_cleared":false,"source_hash":"99995a9a","execution_millis":1,"execution_start":1604264951485},"source":"# Function to create the model and calculate the coherence score\ndef compute_coherence_values(corpus, dictionary, a, b):\n        lda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                              id2word=id2word,\n                                              num_topics=8,\n                                              random_state=100,\n                                              chunksize=100,\n                                              passes=10,\n                                              alpha=a,\n                                              eta=b,\n                                              per_word_topics=True)\n        coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence=\"c_v\")\n        return coherence_model_lda.get_coherence()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the previous notebook we decided to go with 8 being the number of topics, but the graph we previously looked at just showed a peak at 8 and then decreasing after that (we didn't even try 10, well at that moment we didn't it was tried later and showed it was a bit lower than 8, but to get the exact number of best topics we will just try each number from 2 up to 10).\n\nThis next part is going to take a long time, but should be worth it to get the most optimal hyperparameters for this model and dataset.","metadata":{"cell_id":"00016-834cad3e-2902-44a0-8a53-ff4d6a00494d"}},{"cell_type":"code","metadata":{"cell_id":"00017-f23133d4-e797-4f82-9ce9-c7559f6fc675","output_cleared":false,"source_hash":"40b91646","execution_millis":6119061,"execution_start":1604264951497},"source":"import numpy as np\nimport tqdm\n\ngrid = {}\ngrid[\"Validation_Set\"] = {}\n\n# Alpha parameter\nalpha = list(np.arange(0.01, 1, -0.3))\nalpha.append(\"symmetric\")\nalpha.append(\"asymmetric\")\n\n# Beta parameter\nbeta = list(np.arange(0.01, 1, 0.3))\nbeta.append(\"symmetric\")\n\n# Validation sets\nnum_of_docs = len(corpus)\ncorpus_sets = [# gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.25)),\n                # gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.5)),\n                gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n                corpus]\ncorpus_title = [\"75% Corpus\", \"100% Corpus\"]\nmodel_results = {\"Validation_Set\": [],\n                \"Alpha\": [],\n                \"Beta\": [],\n                \"Coherence\": []}\n    \n# iterate through validation corpuses\nfor i in range(len(corpus_sets)):\n    # iterate through alpha values\n    for a in alpha: \n        #iterate though beta values\n        for b in beta:\n            # get the coherence score for the given parameters\n            cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, a=a, b=b)\n            # save the model result\n\n            model_results[\"Validation_Set\"].append(corpus_title[i])\n            model_results[\"Alpha\"].append(a)\n            model_results[\"Beta\"].append(b)\n            model_results[\"Coherence\"].append(cv)\n\npd.DataFrame(model_results).to_csv(\"../lda_tuning_results.csv\", index=False)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best coherence score we were able to get was in this instace:\n\n    Validation_set, Alpha, Beta, Coherence\n    100% Corpus,symmetric,0.9099999999999999,0.42464173686329043\n\nWhich is interesting, because before this testing with our previous notebook we attained a coherence score of 0.44, \nwhich is just shy of it. I think the part that made a difference was in preprocessing our data. For this notebook\nthe bigram and trigram models were slightly changed and common words were not removed. I'm going to work a litle more on \noptimizing the model by trying to get a mallet LDA to work on deepnote (which it wouldn't work on my computer for \nsome odd reason). After optimizing the model, I will go back and try tweaking the preprocessing function and the\nhyperparameters for the bigram and trigram models. \n\nUpdate: The Mallet LDA model doesn't work on this environment either. After spending sometime researching the error, we will just \nuse the normal model. ","metadata":{"tags":[],"cell_id":"00024-d69f4d1d-6c03-4414-8689-a4919a5a1e7b"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00025-225a5d0c-c670-4634-b3fb-2b33a3a636a1"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00026-205e8771-e917-4bed-9e66-02158ba5536f","output_cleared":false,"source_hash":"a885c2dc","execution_millis":290609,"execution_start":1604273822750},"source":"# Create the optimal model\noptimal_lda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                              id2word=id2word,\n                                              num_topics=8,\n                                              random_state=100,\n                                              chunksize=100,\n                                              passes=10,\n                                              alpha=\"symmetric\",\n                                              eta=0.909999999,\n                                              per_word_topics=True)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00026-374f3cd4-f325-4994-9cc7-e9ae807d7fc7","output_cleared":false,"source_hash":"e179eee2","execution_millis":43984,"execution_start":1604275168818},"source":"# Compute coherence score\ncoherence_model_lda = CoherenceModel(model=optimal_lda_model, texts=data_lemmatized, dictionary=id2word, coherence=\"c_v\")\ncoherence_lda = coherence_model_lda.get_coherence()\nprint(\"\\nCoherence score:\", coherence_lda)","execution_count":null,"outputs":[{"name":"stdout","text":"\nCoherence score: 0.44993563970976114\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now we've got a coherence score of 0.45, which is definitely the best we've gotten so far. Now that we've found the optimal model,\nwe can go ahead and work on the preprocessing a bit more and make the data coming in to the model more \"optimal\" to get \"the most\noptimal model\".","metadata":{"tags":[],"cell_id":"00027-91901d62-4577-4ff9-a7b8-dbf5c8c0c40e"}}],"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"deepnote_notebook_id":"9968179e-a3c5-4524-8781-1a12a4953881","deepnote_execution_queue":[]}}